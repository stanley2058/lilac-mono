basePrompt: "You are Lilac."

tools:
  web:
    search:
      provider: tavily

surface:
  # Router behavior is surface-wide (not per-adapter).
  router:
    # Routing modes:
    # - mention: only start a request on mention/reply-to-bot (default for guild channels)
    # - active: treat the channel like a DM (respond to everything)
    defaultMode: mention

    # Per-session routing mode overrides. Key is the session/channel id.
    sessionModes: {}
    # sessionModes:
    #   "123456789012345678":
    #     # Optional: when omitted, this session inherits defaultMode.
    #     # mode: active
    #     # Optional: override the global activeGate.enabled for this channel.
    #     # gate: false
    #     # Optional: override the request model for this session.
    #     # Accepts an alias from models.def or provider/model.
    #     # model: "sonnet"
    #     # Optional: additional session memo entries appended to system prompt.
    #     # Entries can be literal text or file:// URLs.
    #     # additionalPrompts:
    #     #   - "Keep replies concise for this session."
    #     #   - "file:///abs/path/to/session-memo.md"

    # Debounce window (ms) for active mode initial prompt batching.
    activeDebounceMs: 3000

    # Active channel gate to prevent replying to everything.
    # When enabled, we run a lightweight model-based gate before forwarding.
    activeGate:
      enabled: false
      timeoutMs: 2500

  discord:
    tokenEnv: "DISCORD_TOKEN"

    # If both lists are empty, the bot will ignore all Discord traffic.
    allowedChannelIds: []
    allowedGuildIds: []

    # Display name used in attribution + mention stripping. Must not contain spaces.
    botName: "lilac"

    # Optional: override surface DB location (default: <dataDir>/discord-surface.db).
    # dbPath: "/path/to/discord-surface.db"

    # Optional: Discord custom presence/status message.
    # statusMessage: "ready"

    # Discord output rendering mode:
    # - inline: stream directly in the final reply embed (legacy behavior)
    # - preview: stream a transient preview window, then post a full final reply and
    #   delete the preview window
    outputMode: inline

    # Optional: enable Discord reply notifications and user-mention notifications
    # for final output messages. Default is disabled.
    # outputNotification: true

agent:
  # Optional: include token/model timing stats in final response metadata.
  #
  # - false: disable stats metadata on response events (default)
  # - true: enable concise stats line ([M], [T], [TTFT], [TPS])
  # - { verbose: true }: also include input composition ([IC] ...)
  statsForNerds: false

  # Discord-only streaming thinking display lane.
  # - none: disable thinking lane
  # - simple: spinner + elapsed time only
  # - detailed: spinner + elapsed + collapsed reasoning tail (120 chars)
  reasoningDisplay: simple

  subagents:
    # Master switch for delegate-style subagents.
    enabled: true
    # Max delegation depth (primary=0). Default 2 allows self->(explore|general)
    # while explore/general remain non-delegating by policy.
    maxDepth: 2
    # Default timeout applied by subagent.delegate when timeoutMs is omitted.
    defaultTimeoutMs: 180000
    # Hard cap for subagent timeout.
    maxTimeoutMs: 480000
    profiles:
      explore:
        # Which model to run explore subagents on.
        # Can be provider/model or an alias from models.def.
        model: "openrouter/openai/gpt-4o"
        # options:
        #   temperature: 0.0
        # Optional extra prompt fragment appended in explore mode.
        # promptOverlay: |
        #   Focus on codebase mapping, cite concrete file paths, and avoid edits.
      general:
        # General subagent runs with full primary-equivalent tool access.
        # Can be provider/model or an alias from models.def.
        model: "openrouter/openai/gpt-4o"
        # options:
        #   temperature: 0.1
        # Optional extra prompt fragment appended in general mode.
        # promptOverlay: |
        #   Prioritize execution over discussion and report concrete outcomes.
      self:
        # Self subagent is an isolated fork of the primary agent prompt/tooling.
        # Can be provider/model or an alias from models.def.
        model: "openrouter/openai/gpt-4o"
        # options:
        #   temperature: 0.2
        # Optional extra prompt fragment appended in self mode.
        # promptOverlay: |
        #   Assume no prior conversation context; restate assumptions briefly.

# Optional: extra system prompt text prepended to data/prompts/*.
# basePrompt: |
#   You are lilac.

models:
  # Optional preset registry to make model selection ergonomic.
  #
  # def:
  #   sonnet:
  #     model: "openrouter/anthropic/claude-sonnet-4.5"
  #     options:
  #       anthropic:
  #         thinking:
  #           type: enabled
  #       gateway:
  #         order: [anthropic, vertex, bedrock]

  main:
    # Can be either a full spec (provider/model) or an alias from models.def.
    model: "openrouter/openai/gpt-4o"
    # For non-official OpenAI-compatible upstreams, use:
    # model: "openai-compatible/your-model-id"
    # options:
    #   temperature: 0.2
    #   # For openai/codex providers, parallelToolCalls defaults to true.
    #   # Set false to force sequential provider-side tool calling.
    #   openai:
    #     parallelToolCalls: false
    #   # openai-compatible provider options use the openaiCompatible namespace.
    #   openaiCompatible:
    #     temperature: 0.2

  # Fast/cheap model for lightweight features (router gate, etc.)
  fast:
    model: "openrouter/openai/gpt-4o-mini"
    # options:
    #   temperature: 0.0

# Entity aliasing / mention rewriting (Discord only).
#
# This powers two conveniences:
# - outbound: "@Stanley" -> "<@123>" and "#ops" -> "<#456>"
# - inbound: "<@123>" -> "@Stanley" and "<#456>" -> "#ops"
#
# Notes:
# - Matches are case-insensitive; the canonical casing is preserved from config.
# - Tokens are sanitized (whitespace -> '_' and leading @/# stripped).
# entity:
#   users:
#     Stanley:
#       discord: "123456789012345678"
#
#   sessions:
#     discord:
#       ops: "234567890123456789"
